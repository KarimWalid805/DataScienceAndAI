{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarimWalid805/MSDS680-ML-COLAB/blob/main/CourseProject_MSDS680_KarimWalid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning (MSDS680) Course Project**\n",
        "This projects aims to find the best performing model that can train on the EMNIST letters data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Author: Karim Walid\n",
        "*   Date: 30/03/2025\n",
        "*   Instructor: Pantelis Kaplanoglou\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YTpVkDaO4bgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 01**"
      ],
      "metadata": {
        "id": "HL64FvK34tMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Rapid Deep Neural Networks library\n",
        "!pip install radnn -q\n",
        "!pip list | grep \"radnn\"\n",
        "\n",
        "from radnn.system.hosts import ColabHost\n",
        "\n",
        "# Mount to the project folder that is under the lesson folder\n",
        "ColabHost().detect_workspace([\"CS345\", \"MSDS680\"]).change_to_project_dir(\"MLData\")"
      ],
      "metadata": {
        "id": "Pepd9fW_xTj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"]=\"1\"   # Use Keras 2.x. We set this before the first import of tensorflow\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from radnn import mlsys, FileSystem\n",
        "from radnn.experiment import MLExperimentConfig\n",
        "#from mllib.visualization import CPlot, CPlotTrainingLogs, CAutoMultiImagePlot\n",
        "oFileSys = FileSystem(config_folder=\"MLConfig\", model_folder=\"MLModels\", dataset_folder=\"MLData\")\n",
        "# __________ | Settings | __________\n",
        "IS_PLOTING_DATA             = True\n",
        "IS_RETRAINING               = False\n",
        "RANDOM_SEED             = 2022\n",
        "mlsys.filesys = FileSystem(\"MLConfig\", model_folder=\"MLModels\", dataset_folder=\"MLData\")"
      ],
      "metadata": {
        "id": "4SCRH650xXi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "DB6QaZBZqrSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "caYoWs_GwQcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "# =========================================================================================================================\n",
        "class SetType(Enum):\n",
        "  TRAINING_SET      = 1\n",
        "  VALIDATION_SET    = 2\n",
        "  UNKNOWN_TEST_SET  = 3\n",
        "# =========================================================================================================================\n",
        "\n",
        "\n",
        "# =========================================================================================================================\n",
        "class CDataSet(object):\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  # Constructor\n",
        "  def __init__(self):\n",
        "    # ................................................................\n",
        "    # // Fields \\\\\n",
        "    self.Samples            = None\n",
        "    self.Labels             = None\n",
        "    self.SampleCount        = 0\n",
        "    self.FeatureCount       = None\n",
        "    self.ClassCount         = None\n",
        "\n",
        "    self.TSSamples      = None\n",
        "    self.TSLabels       = None\n",
        "    self.TSSampleCount  = 0\n",
        "\n",
        "    self.VSSamples      = None\n",
        "    self.VSLabels       = None\n",
        "    self.VSSampleCount  = 0\n",
        "\n",
        "    self.USSamples      = None\n",
        "    self.USLabels       = None\n",
        "    self.USSampleCount  = 0\n",
        "    # ................................................................\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  # Method\n",
        "  def DebugPrint(self):\n",
        "    print(\"Shape of sample tensor\", self.Samples.shape)\n",
        "    print('.'*80)\n",
        "\n",
        "    print(\"Datatype of sample tensor before convertion: %s\" % str(self.Samples.dtype))\n",
        "    # Convert the data to 32bit floating point numbers (default for faster computations)\n",
        "    self.Samples = np.asarray(self.Samples, dtype=np.float32)\n",
        "    print(\"Datatype of sample tensor after convertion: %s\" % str(self.Samples.dtype))\n",
        "    print('.'*80)\n",
        "\n",
        "    # Classification into 2 classes == Binary classification\n",
        "    print(\"Class labels\")\n",
        "    print(self.Labels)\n",
        "    print('.'*80)\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  def LoadSet(self, p_nSamples, p_nLabels, p_nType):\n",
        "    if p_nType == SetType.TRAINING_SET:\n",
        "      self.TSSamples = p_nSamples\n",
        "      self.TSLabels  = p_nLabels\n",
        "      self.TSSampleCount = self.TSSamples.shape[0]\n",
        "      if self.FeatureCount is None:\n",
        "        self.FeatureCount = self.TSSamples.shape[1]\n",
        "        self.ClassCount = len(np.unique(self.TSLabels))\n",
        "    elif p_nType == SetType.VALIDATION_SET:\n",
        "      self.VSSamples = p_nSamples\n",
        "      self.VSLabels = p_nLabels\n",
        "      self.VSSampleCount = self.VSSamples.shape[0]\n",
        "\n",
        "    self.SampleCount = self.TSSampleCount + self.VSSampleCount + self.USSampleCount\n",
        "# ========================================================================================================================="
      ],
      "metadata": {
        "id": "Tg9OMIvrw0aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ag63l5bRqyNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "\n",
        "class EMNISTDataSet(CDataSet):\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  # Constructor\n",
        "  def __init__(self, filestore):\n",
        "    super(EMNISTDataSet, self).__init__()\n",
        "\n",
        "    with gzip.open(filestore.file(\"emnist-letters-test-images-idx3-ubyte.gz\"), 'rb') as f:\n",
        "        dTestImages = np.frombuffer(f.read(), np.uint8, offset = 16).reshape(-1, 28 ,28, 1)\n",
        "\n",
        "    with gzip.open(filestore.file(\"emnist-letters-train-images-idx3-ubyte.gz\"), 'rb') as f:\n",
        "        dTrainImages = np.frombuffer(f.read(), np.uint8, offset = 16).reshape(-1, 28 ,28, 1)\n",
        "\n",
        "    with gzip.open(filestore.file(\"emnist-letters-test-labels-idx1-ubyte.gz\"), 'rb') as f:\n",
        "        dTestLabels = np.frombuffer(f.read(), np.uint8, offset = 8)\n",
        "\n",
        "    with gzip.open(filestore.file(\"emnist-letters-train-labels-idx1-ubyte.gz\"), 'rb') as f:\n",
        "        dTrainLabels = np.frombuffer(f.read(), np.uint8, offset = 8)\n",
        "\n",
        "\n",
        "    self.LoadSet(dTrainImages, dTrainLabels, SetType.TRAINING_SET)\n",
        "    self.LoadSet(dTestImages, dTestLabels, SetType.VALIDATION_SET)\n",
        "  # --------------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "EViTk5EFF7Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _____// Data Hyperparameters \\\\_____\n",
        "oDatasetFS = oFileSys.datasets.subfs(\"EMNIST\")\n",
        "\n",
        "\n",
        "oEMNIST  = EMNISTDataSet(oDatasetFS)\n",
        "sDataName  = \"EMNIST\"\n",
        "nSamples   = oEMNIST.TSSamples\n",
        "nLabels    = oEMNIST.TSLabels\n",
        "\n",
        "vSamples = oEMNIST.VSSamples\n",
        "vLabels    = oEMNIST.VSLabels\n",
        "\n",
        "print(\"Loaded %s dataset\" % sDataName)\n",
        "print(\"Training set shape:\", nSamples.shape)\n",
        "print(\"Class count:\", len(np.unique(nLabels)))"
      ],
      "metadata": {
        "id": "Uzk1xLKFq0HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oDatasetFS.obj.save(nSamples, \"nSamples.pkl\")\n",
        "oDatasetFS.obj.save(nLabels, \"nLabels.pkl\")\n",
        "oDatasetFS.obj.save(vSamples, \"vSamples.pkl\")\n",
        "oDatasetFS.obj.save(vLabels, \"vLabels.pkl\")"
      ],
      "metadata": {
        "id": "gKjL-AR9EoKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 02**"
      ],
      "metadata": {
        "id": "2P--Wb3Z5Yr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three options of traning CNN01(worst),CNN02(better) and CNN03(best)"
      ],
      "metadata": {
        "id": "JTfDDU7wWayl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_CNN_01 = MLExperimentConfig(number=1).assign({\n",
        "                 \"ModelName\": \"EMNIST_CNN\"\n",
        "                ,\"CNN.InputShape\": [28,28,1]\n",
        "                ,\"CNN.Classes\": 26\n",
        "                ,\"CNN.ModuleCount\": 6\n",
        "                ,\"CNN.ConvOutputFeatures\": [9,16,24,32,48,48]\n",
        "                ,\"CNN.ConvWindows\": [ [3,2,True], [3,1,True] ,  [3,1,True], [3,2,True], [3,1,True], [3,1,True] ]\n",
        "                ,\"CNN.PoolWindows\": [  None      , None       ,  None      , None      , [3,2]     , None      ]\n",
        "                ,\"CNN.HasBatchNormalization\": True\n",
        "                ,\"Training.MaxEpoch\": 12\n",
        "                ,\"Training.BatchSize\": 500\n",
        "                ,\"Training.LearningRate\": 0.001\n",
        "                ,\"Experiment.RandomSeed\": 2022\n",
        "            })\n",
        "\n",
        "oConfig = CONFIG_CNN_01"
      ],
      "metadata": {
        "id": "sLfx7JXAE5pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_CNN_02 = MLExperimentConfig(number=2).assign({\n",
        "     \"ModelName\": \"Better_MNIST_CNN\"\n",
        "    ,\"CNN.InputShape\": [28,28,1]\n",
        "    ,\"CNN.Classes\": 26\n",
        "    ,\"CNN.ModuleCount\": 8  # Increased depth\n",
        "    ,\"CNN.ConvOutputFeatures\": [16, 32, 48, 64, 128, 128, 256, 256]  # More feature maps\n",
        "    ,\"CNN.ConvWindows\": [ [3,2,True], [3,1,True], [5,1,True], [3,2,True], [3,1,True], [3,1,True], [3,1,True], [3,2,True] ]\n",
        "    ,\"CNN.PoolWindows\": [  [2,2] , None , [2,2], None , None , [2,2], None , None]  # More pooling\n",
        "    ,\"CNN.HasBatchNormalization\": True\n",
        "    ,\"CNN.HasDropout\": True  # Adding dropout for regularization\n",
        "    ,\"CNN.DropoutRate\": 0.3  # Dropout rate at 30%\n",
        "    ,\"Training.MaxEpoch\": 20  # More epochs for better convergence\n",
        "    ,\"Training.BatchSize\": 256  # Lower batch size for better generalization\n",
        "    ,\"Training.LearningRate\": 0.001\n",
        "    ,\"Training.LearningRateDecay\": 0.95  # Decay learning rate over epochs\n",
        "    ,\"Experiment.RandomSeed\": 2022\n",
        "})\n",
        "\n",
        "oConfig = CONFIG_CNN_02"
      ],
      "metadata": {
        "id": "noJBkA1OqSjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_CNN_03 = MLExperimentConfig(number=3).assign({\n",
        "     \"ModelName\": \"Best_MNIST_CNN\"\n",
        "    ,\"CNN.InputShape\": [28,28,1]\n",
        "    ,\"CNN.Classes\": 26\n",
        "    ,\"CNN.ModuleCount\": 8  # Increased depth\n",
        "    ,\"CNN.ConvOutputFeatures\": [16, 32, 48, 64, 128, 128, 256, 256, 512, 512]  # less feature maps\n",
        "    ,\"CNN.ConvWindows\": [ [3,2,True], [3,1,True], [5,1,True], [3,2,True], [3,1,True], [3,1,True], [3,1,True], [3,2,True] ]\n",
        "    ,\"CNN.PoolWindows\": [  [2,2] , None , [2,2], None , None , [2,2], None , None]  # More pooling\n",
        "    ,\"CNN.HasBatchNormalization\": True\n",
        "    ,\"CNN.HasDropout\": True  # Adding dropout for regularization\n",
        "    ,\"CNN.DropoutRate\": 0.3  # Dropout rate at 30%\n",
        "    ,\"Training.MaxEpoch\": 20  # More epochs for better convergence\n",
        "    ,\"Training.BatchSize\": 128 # Lower batch size for better generalization\n",
        "    ,\"Training.LearningRate\": 0.0001\n",
        "    ,\"Training.LearningRateDecay\": 0.90  # Decay learning rate over epochs\n",
        "    ,\"Experiment.RandomSeed\": 2022\n",
        "})\n",
        "\n",
        "oConfig = CONFIG_CNN_03"
      ],
      "metadata": {
        "id": "jKFRC27Lu6Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from radnn.data import DataSetBase\n",
        "\n",
        "\n",
        "class MNISTDataFeed(object):\n",
        "\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  def __init__(self, p_oDataSet, p_oConfig, p_nFeatureCount=None):\n",
        "    super(MNISTDataFeed, self).__init__()\n",
        "    # ................................................................\n",
        "\n",
        "    # // Fields \\\\\n",
        "    self.DataSet: CDataSet  = p_oDataSet\n",
        "    self.FileStore          = oFileSys.datasets.subfs(\"EMNIST\")\n",
        "    self.PredictBatchSize = None\n",
        "    if \"Prediction.BatchSize\" in p_oConfig:\n",
        "      self.PredictBatchSize   = p_oConfig[\"Prediction.BatchSize\"]\n",
        "    self.TrainingBatchSize  = p_oConfig[\"Training.BatchSize\"]\n",
        "\n",
        "    # Calculate mean and std\n",
        "    oDataSetStats = self.FileStore.obj.load(\"EMNIST-meanstd.pkl\")\n",
        "    if oDataSetStats is not None:\n",
        "      self.PixelMean    = oDataSetStats[\"mean\"]\n",
        "      self.PixelStd     = oDataSetStats[\"std\"]\n",
        "    else:\n",
        "      self.calculateAndSaveDatasetStats()\n",
        "\n",
        "    if \"ClassCount\" in p_oConfig:\n",
        "      self.ClassCount = p_oConfig[\"ClassCount\"]\n",
        "    else:\n",
        "      self.ClassCount = p_oConfig[\"CNN.Classes\"]\n",
        "    if \"InputShape\" in p_oConfig:\n",
        "      self.InputShape = p_oConfig[\"InputShape\"]\n",
        "    else:\n",
        "      self.InputShape = p_oConfig[\"CNN.InputShape\"]\n",
        "    self.PaddingOffset = 3\n",
        "    self.PaddingTarget = self.InputShape[0] + 3\n",
        "\n",
        "\n",
        "    self.TSFeed       = self.CreateTrainingDataFeed((  self.DataSet.TSSamples\n",
        "                                                      ,self.DataSet.TSLabels), self.TrainingBatchSize)\n",
        "    self.TSRecallFeed = None\n",
        "    self.VSFeed       = self.CreateValidationDataFeed((self.DataSet.VSSamples,\n",
        "                                                        self.DataSet.VSLabels), 100)\n",
        "    # ................................................................\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  def calculateAndSaveDatasetStats(self):\n",
        "    self.PixelMean = np.mean(self.DataSet.TSSamples, axis=(0,1,2))\n",
        "    self.PixelStd  = np.std(self.DataSet.TSSamples, axis=(0,1,2))\n",
        "    oDataSetStats = { \"mean\": self.PixelMean, \"std\": self.PixelStd}\n",
        "    self.FileStore.obj.save(oDataSetStats,\"EMNIST-meanstd.pkl\")\n",
        "  # --------------------------------------------------------------------------------------------------------\n",
        "  def denormalize_image(self, normed_image):\n",
        "      image = (normed_image * self.PixelStd) + self.PixelMean\n",
        "      image = image.astype(np.uint8)\n",
        "      return image\n",
        "  # --------------------------------------------------------------------------------------------------------\n",
        "  def normalizeImage(self, p_nImage):\n",
        "      normed_image = (p_nImage - self.PixelMean) / self.PixelStd\n",
        "      return normed_image\n",
        "  # --------------------------------------------------------------------------------------------------------\n",
        "  def randomCrop(self, image):\n",
        "      # CIFAR: We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side,\n",
        "      # and a  32Ã—32  crop is  randomly  sampled  from  the  paddedimage or its horizontal flip.\n",
        "\n",
        "      #distorted_image = image\n",
        "      distorted_image = tf.image.pad_to_bounding_box(image, self.PaddingOffset, self.PaddingOffset\n",
        "                                                          ,  self.PaddingTarget,  self.PaddingTarget)    # pad 3 pixels to each side\n",
        "      distorted_image = tf.image.random_crop(distorted_image, self.InputShape)\n",
        "      # [WARNING] Flip is a non-label preserving transformation for ExShapes\n",
        "      # distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
        "      return distorted_image\n",
        "  # -----------------------------------------------------------------------------------\n",
        "  def PreprocessTrainingImageAugmentDataset(self, p_tImageInTS, p_tLabelInTS):\n",
        "    # Original feed that was used in experiments did not contain the following line but the bug was not triggered (cached image format float32?)\n",
        "    tImage = tf.cast(p_tImageInTS, tf.float32)  # //[BF] overflow of standardization\n",
        "    tNormalizedImage = self.normalizeImage(tImage)\n",
        "\n",
        "    tNewRandomImage  = self.randomCrop(tNormalizedImage)\n",
        "\n",
        "    tTargetOneHot = tf.one_hot(p_tLabelInTS, self.ClassCount)\n",
        "\n",
        "    return tNewRandomImage, tTargetOneHot\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  def CreateTrainingDataFeed(self, p_oDataTuple, p_nBatchSize):\n",
        "    oTSData = tf.data.Dataset.from_tensor_slices(p_oDataTuple)\n",
        "    oTSData = oTSData.map(self.PreprocessTrainingImageAugmentDataset, num_parallel_calls=8)\n",
        "    #oTSData = oTSData.cache() # This reduced accuracy on the cifar10 data feed\n",
        "    oTSData = oTSData.shuffle(self.DataSet.TSSampleCount)\n",
        "    oTSData = oTSData.batch(p_nBatchSize)\n",
        "    #oTSData = oTSData.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    print(\"Training data feed object:\", oTSData)\n",
        "    return oTSData\n",
        "  # -----------------------------------------------------------------------------------\n",
        "  def PreprocessValidationImageWithID(self, p_tImageID, p_tImageInVS, p_tLabelInVS):\n",
        "    tImage = tf.cast(p_tImageInVS, tf.float32)  # //[BF] overflow of standardization\n",
        "    tNormalizedImage = self.normalizeImage(tImage)\n",
        "\n",
        "    tTargetOneHot = tf.one_hot(p_tLabelInVS, self.ClassCount)\n",
        "\n",
        "    return (p_tImageID, tNormalizedImage), tTargetOneHot\n",
        "  # -----------------------------------------------------------------------------------\n",
        "  def PreprocessValidationImage(self, p_tImageInVS, p_tLabelInVS):\n",
        "    tImage = tf.cast(p_tImageInVS, tf.float32)  # //[BF] overflow of standardization\n",
        "    tNormalizedImage = self.normalizeImage(tImage)\n",
        "\n",
        "    tTargetOneHot = tf.one_hot(p_tLabelInVS, self.ClassCount)\n",
        "\n",
        "    return tNormalizedImage, tTargetOneHot\n",
        "  # -----------------------------------------------------------------------------------\n",
        "  def CreateValidationDataFeed(self, p_oDataTuple, p_nBatchSize=None):\n",
        "    nArgsCount = len(list(p_oDataTuple))\n",
        "    if nArgsCount == 2:\n",
        "      oData = tf.data.Dataset.from_tensor_slices(p_oDataTuple)\n",
        "      oData = oData.map(self.PreprocessValidationImage, num_parallel_calls=8)\n",
        "    elif nArgsCount == 3:\n",
        "      oData = tf.data.Dataset.from_tensor_slices(p_oDataTuple)\n",
        "      oData = oData.map(self.PreprocessValidationImageWithID, num_parallel_calls=8)\n",
        "\n",
        "    if p_nBatchSize is None:\n",
        "      p_nBatchSize = self.DataSet.vs_sample_count\n",
        "    oData = oData.batch(p_nBatchSize)\n",
        "    print(\"Validation data feed object:\", oData)\n",
        "    return oData\n",
        "  # --------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "HrJu1hN_Gy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oData = MNISTDataFeed(oEMNIST, oConfig)\n",
        "\n",
        "print(\"Training data feed object:\", oData.TSFeed)\n",
        "print(\"Validation data feed object:\", oData.VSFeed)"
      ],
      "metadata": {
        "id": "_KyfogI3GzMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import InputLayer, Flatten, Dense, BatchNormalization, Activation, Softmax\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.regularizers import L2\n",
        "# =========================================================================================================================\n",
        "class CCNNCustom(keras.Model):\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  # Constructor\n",
        "  def __init__(self, p_oConfig):\n",
        "    super(CCNNCustom, self).__init__()\n",
        "\n",
        "    # ..................... Object Attributes ...........................\n",
        "    self.Config = p_oConfig\n",
        "\n",
        "    self.InputShape         = self.Config[\"CNN.InputShape\"]\n",
        "    self.ClassCount         = self.Config[\"CNN.Classes\"]\n",
        "    self.ModuleCount        = self.Config[\"CNN.ModuleCount\"]\n",
        "\n",
        "    self.ConvLayerFeatures  = self.Config[\"CNN.ConvOutputFeatures\"]\n",
        "    self.ConvWindows        = self.Config[\"CNN.ConvWindows\"]\n",
        "    self.PoolWindows        = self.Config[\"CNN.PoolWindows\"]\n",
        "\n",
        "    if \"CNN.HasBatchNormalization\" not in self.Config:\n",
        "        self.Config[\"CNN.HasBatchNormalization\"] = False\n",
        "\n",
        "    self.KerasLayers        = []\n",
        "\n",
        "    self.OutputLayer        = None\n",
        "    self.SoftmaxActivation  = None\n",
        "    self.Input              = None\n",
        "    self.Structure          = None\n",
        "    # ...................................................................\n",
        "\n",
        "    # Default values for extra customization\n",
        "\n",
        "    if \"CNN.ActivationFunction\" not in self.Config:\n",
        "        self.Config[\"CNN.ActivationFunction\"] = \"relu\"\n",
        "\n",
        "    if \"CNN.ConvHasBias\" not in self.Config:\n",
        "        self.Config[\"CNN.ConvHasBias\"] = False\n",
        "\n",
        "    if \"CNN.KernelInitializer\" not in self.Config:\n",
        "        self.Config[\"CNN.KernelInitializer\"] = \"glorot_uniform\"\n",
        "\n",
        "    if \"CNN.BiasInitializer\" not in self.Config:\n",
        "        self.Config[\"CNN.BiasInitializer\"] = \"zeros\"\n",
        "\n",
        "    if \"Training.RegularizeL2\" not in self.Config:\n",
        "        self.Config[\"Training.RegularizeL2\"] = False\n",
        "\n",
        "    if \"Training.WeightDecay\" not in self.Config:\n",
        "        self.Config[\"Training.WeightDecay\"] =  1e-5\n",
        "\n",
        "    if self.Config[\"Training.RegularizeL2\"]:\n",
        "        print(\"Using L2 regularization of weights with weight decay %.6f\" % self.Config[\"Training.WeightDecay\"])\n",
        "\n",
        "\n",
        "    self.Create()\n",
        "  # --------------------------------------------------------------------------------------\n",
        "  def Create(self):                # override a virtual in our base class\n",
        "    # This loop creates stacked convolutional modules of the form   CONVOLUTION - ACTIVATION - NORMALIZATION - MAX POOLING\n",
        "    for nModuleIndex in range(0, self.ModuleCount):\n",
        "      nFeatures     = self.ConvLayerFeatures[nModuleIndex]\n",
        "      oConvWindowSetup = self.ConvWindows[nModuleIndex]\n",
        "      nWindowSize   = oConvWindowSetup[0]\n",
        "      nStride       = oConvWindowSetup[1]\n",
        "\n",
        "      sPaddingType      = \"valid\"\n",
        "      if len(oConvWindowSetup) == 3:\n",
        "          bIsPadding    = oConvWindowSetup[2]\n",
        "          if bIsPadding:\n",
        "              sPaddingType = \"same\"\n",
        "\n",
        "      if self.Config[\"Training.RegularizeL2\"]:\n",
        "          oWeightRegularizer = L2(self.Config[\"Training.WeightDecay\"])\n",
        "      else:\n",
        "          oWeightRegularizer = None\n",
        "\n",
        "      oConvolution = Conv2D(nFeatures, kernel_size=nWindowSize, strides=nStride, padding=sPaddingType\n",
        "                            , use_bias=self.Config[\"CNN.ConvHasBias\"]\n",
        "                            , kernel_regularizer=oWeightRegularizer\n",
        "                            , kernel_initializer=self.Config[\"CNN.KernelInitializer\"]\n",
        "                            , bias_initializer=self.Config[\"CNN.BiasInitializer\"])\n",
        "      self.KerasLayers.append(oConvolution)\n",
        "\n",
        "      oActivation  = Activation(self.Config[\"CNN.ActivationFunction\"])\n",
        "      self.KerasLayers.append(oActivation)\n",
        "\n",
        "      if self.Config[\"CNN.HasBatchNormalization\"]:\n",
        "          oNormalization = BatchNormalization()\n",
        "          self.KerasLayers.append(oNormalization)\n",
        "\n",
        "      oPoolWindow   = self.PoolWindows[nModuleIndex]\n",
        "      # Set the pool size to None for a module that does not do Max Pooling.\n",
        "      if oPoolWindow is not None:\n",
        "          nPoolSize   = oPoolWindow[0]\n",
        "          nPoolStride = oPoolWindow[1]\n",
        "          oMaxPooling = MaxPooling2D(pool_size=[nPoolSize, nPoolSize], strides=[nPoolStride, nPoolStride])\n",
        "          self.KerasLayers.append(oMaxPooling)\n",
        "\n",
        "\n",
        "    # After the stack of convolutional modules, the activation cube will be flattened to a vector using a Flatten keras layer\n",
        "    self.FlatteningLayer = Flatten()\n",
        "\n",
        "\n",
        "    # The output layer for the classifier is a fully connected (dense) that has one neuron for each class.\n",
        "    # You might consider the stack of convolutional modules functioning as the \"hidden\" layer in the 2-layer NN architecture.\n",
        "    if self.Config[\"Training.RegularizeL2\"]:\n",
        "        oWeightRegularizer = L2(self.Config[\"Training.WeightDecay\"])\n",
        "    else:\n",
        "        oWeightRegularizer = None\n",
        "    self.OutputLayer = Dense(self.ClassCount, use_bias=True\n",
        "                             ,kernel_regularizer=oWeightRegularizer )\n",
        "\n",
        "    # Instead of using sigmoid for each neuron, we use the softmax activation function so that neuron \"fire\" together.\n",
        "    self.SoftmaxActivation = Softmax()\n",
        "  # --------------------------------------------------------------------------------------------------------\n",
        "  def call(self, p_tInput):        # overrides a virtual in keras.Model class\n",
        "    bPrint = self.Structure is None\n",
        "    if bPrint:\n",
        "        self.Structure = []\n",
        "\n",
        "    self.Input = p_tInput\n",
        "\n",
        "    # ....... Convolutional Feature Extraction  .......\n",
        "    # Feed forward to the next layer\n",
        "    tA = p_tInput\n",
        "    for nIndex,oKerasLayer in enumerate(self.KerasLayers):\n",
        "        if bPrint:\n",
        "            self.Structure.append([nIndex + 1, str(tA.name), str(tA.shape)])\n",
        "        tA = oKerasLayer(tA)\n",
        "\n",
        "    # Flattens the activation cube to a vector\n",
        "    tA = self.FlatteningLayer(tA)\n",
        "    if bPrint:\n",
        "        nIndex += 1\n",
        "        self.Structure.append([nIndex + 1, str(tA.name), str(tA.shape)])\n",
        "\n",
        "    # ....... Classifier  .......\n",
        "    # Fully connected (dense) layer that has a count of neurons equal to the classes, with softmax activation function\n",
        "    tA = self.OutputLayer(tA)\n",
        "    if bPrint:\n",
        "        nIndex += 1\n",
        "        self.Structure.append([nIndex + 1, str(tA.name), str(tA.shape)])\n",
        "\n",
        "    tA = self.SoftmaxActivation(tA)\n",
        "    if bPrint:\n",
        "        nIndex += 1\n",
        "        self.Structure.append([nIndex + 1, str(tA.name), str(tA.shape)])\n",
        "\n",
        "\n",
        "    return tA\n",
        "  # --------------------------------------------------------------------------------------\n",
        "# ========================================================================================================================="
      ],
      "metadata": {
        "id": "iSPuuOscq7Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oConfig = CONFIG_CNN_03 #change the model here - Options: (\"CONFIG_CNN_01\"/\"CONFIG_CNN_02\"/\"CONFIG_CNN_03\")\n",
        "oCNN = CCNNCustom(oConfig)\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "def LRSchedule(epoch, lr):\n",
        "    if epoch == 10:\n",
        "        nNewLR = lr * 0.5\n",
        "        print(\"Setting LR to %.5f\" % nNewLR)\n",
        "        return nNewLR\n",
        "    else:\n",
        "        return lr\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "nInitialLearningRate    = oConfig[\"Training.LearningRate\"]\n",
        "\n",
        "oCostFunction  = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "oOptimizer = tf.keras.optimizers.Adam(learning_rate=nInitialLearningRate)\n",
        "oCallbacks = None"
      ],
      "metadata": {
        "id": "t701KdDz4cJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-------------------- Experiment Configuration ------------------\") #Check which one you chose\n",
        "print(oConfig)"
      ],
      "metadata": {
        "id": "L0vL1nNy5LAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from radnn import mlsys\n",
        "mlsys.random_seed_all(oConfig[\"Experiment.RandomSeed\"])\n",
        "print(oConfig)"
      ],
      "metadata": {
        "id": "2-GIQXm25-9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NK7VS2ocKm4"
      },
      "outputs": [],
      "source": [
        "IS_RETRAINING = True\n",
        "\n",
        "oModelFS = oFileSys.models.subfs(oConfig.experiment_code)\n",
        "sModelFolder = oModelFS.base_folder\n",
        "# Append the .keras extension to the model folder path\n",
        "sModelFolder = sModelFolder + \".keras\"  # or sModelFolder + \".h5\" for HDF5 format\n",
        "sProcessLogFileName = \"train.history\"\n",
        "\n",
        "if (not os.path.isdir(sModelFolder)) or IS_RETRAINING:\n",
        "  oCNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=[\"accuracy\"])\n",
        "  oProcessLog = oCNN.fit(  oData.TSFeed, batch_size=oConfig[\"Training.BatchSize\"]\n",
        "                            ,epochs=oConfig[\"Training.MaxEpoch\"]\n",
        "                            ,validation_data=oData.VSFeed\n",
        "                          )\n",
        "  print(f\"Saving to {sModelFolder}\")\n",
        "  oCNN.save(sModelFolder)\n",
        "  oHistory = oProcessLog.history\n",
        "  oModelFS.obj.save(oHistory, sProcessLogFileName, is_overwriting=True)\n",
        "else:\n",
        "  print(f\"Loading from {sModelFolder}\")\n",
        "  oCNN = tf.keras.models.load_model(sModelFolder)\n",
        "  oHistory = oModelFS.obj.load(sProcessLogFileName)\n",
        "oCNN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "sFileName = oModelFS.file(f\"Model-Structure-{oConfig.experiment_code}.csv\")\n",
        "\n",
        "with open(sFileName, \"w\") as f:\n",
        "    write = csv.writer(f)\n",
        "    for oItem in oCNN.Structure:\n",
        "        print(oItem)\n",
        "        write.writerow(oItem)"
      ],
      "metadata": {
        "id": "xKa7isPR9SWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "2s-Zz6TE6FH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from radnn.plots import PlotLearningCurve\n",
        "\n",
        "sPlotFileName = oFileSys.models.file(f'{oConfig.experiment_code}-LearningCurve-%s.png')\n",
        "oPlot = PlotLearningCurve(oHistory, oConfig.experiment_code)\n",
        "oPlot.prepare().save(sPlotFileName % \"Accuracy\").show()\n",
        "oPlot.prepare_cost(\"CCE\").save(sPlotFileName % \"CCE\").show()"
      ],
      "metadata": {
        "id": "47KC1TKn90oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nPredictedProbabilities = oCNN.predict(oData.VSFeed)\n",
        "nPredictedClassLabels  = np.argmax(nPredictedProbabilities, axis=1)\n",
        "\n",
        "nTargetClassLabels     = oEMNIST.VSLabels\n",
        "\n",
        "for nIndex, nProbs in enumerate(nPredictedProbabilities):\n",
        "  if nIndex < 10:\n",
        "    print(\"#%.2d Predicted:%d (Probabilities:%s) Actual:%d\" % (nIndex+1, nPredictedClassLabels[nIndex], nProbs, nTargetClassLabels[nIndex])) # [PYTHON] Format string example\n",
        "    print(\"  |__ Sum of all output neuron activations:%.3f\" % np.sum(nProbs))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4fR580auIVf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from radnn.evaluation import EvaluateClassification\n",
        "\n",
        "# We create an evaluator object that will produce several metrics\n",
        "oEvaluator = EvaluateClassification(nTargetClassLabels, nPredictedClassLabels)\n",
        "oEvaluator.print_per_class()\n",
        "oEvaluator.print_overall()\n",
        "oEvaluator.print_confusion_matrix()"
      ],
      "metadata": {
        "id": "uL-sQX0W9PFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}